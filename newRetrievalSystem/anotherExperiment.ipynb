{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-27 11:46:53--  https://files.webis.de/software/pyterrier-plugins/custom-terrier-token-processing-1.0-SNAPSHOT-jar-with-dependencies.jar\n",
      "Resolving files.webis.de (files.webis.de)... 141.54.132.200\n",
      "Connecting to files.webis.de (files.webis.de)|141.54.132.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 499865236 (477M) [application/java-archive]\n",
      "Saving to: ‘/root/.pyterrier/custom-terrier-token-processing-0.0.1.jar’\n",
      "\n",
      "/root/.pyterrier/cu 100%[===================>] 476.71M  96.6MB/s    in 5.5s    \n",
      "\n",
      "2024-06-27 11:46:59 (87.1 MB/s) - ‘/root/.pyterrier/custom-terrier-token-processing-0.0.1.jar’ saved [499865236/499865236]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://files.webis.de/software/pyterrier-plugins/custom-terrier-token-processing-1.0-SNAPSHOT-jar-with-dependencies.jar -O /root/.pyterrier/custom-terrier-token-processing-0.0.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "#from jnius import autoclass\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "#'mam10eks:custom-terrier-token-processing:0.0.1'\n",
    "#'com.github.terrierteam:terrier-prf:-SNAPSHOT'\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=['mam10eks:custom-terrier-token-processing:0.0.1'])\n",
    "    from jnius import autoclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= [\"I\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n",
    "              \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \n",
    "              \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \n",
    "              \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \n",
    "              \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "                \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\",\n",
    "                  \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\",\n",
    "                    \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \n",
    "                    \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\",\n",
    "                      \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \n",
    "                      \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "                        \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \n",
    "                        \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:  71%|███████   | 90006/126958 [00:26<00:05, 6575.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:47:36.039 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (2020.mir_conference-2020.1) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents: 100%|██████████| 126958/126958 [00:32<00:00, 3897.61it/s] \n",
      "100%|██████████| 126958/126958 [00:32<00:00, 3897.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:47:46.717 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 3 empty documents\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "stemmer = pt.TerrierStemmer.porter\n",
    "stemmer1= pt.TerrierStemmer.weakporter\n",
    "tokenizer = pt.TerrierTokeniser.english\n",
    "\n",
    "#lemmatizer = autoclass(\"org.terrier.terms.StanfordLemmatizer\")()\n",
    "#stemmer2 = autoclass(\"org.terrier.terms.LemurKrovetzStemmer\")\n",
    "\n",
    "iter_indexer = pt.IterDictIndexer(\"./passage_index\", overwrite=True, verbose= True, tokeniser=tokenizer, stemmer =stemmer, meta={'docno': 100, 'text': 4096})\n",
    "#iter_indexer = pt.IterDictIndexer(\"./passage_index\", overwrite=True, verbose= True, stopwords =None, stemmer =None, meta={'docno': 100, 'text': 4096})\n",
    "\n",
    "print('start')\n",
    "indexref = iter_indexer.index(tqdm(pt_dataset.get_corpus_iter()))\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pt.BatchRetrieve(indexref, wmodel=\"TF_IDF\")\n",
    "bm25 = pt.BatchRetrieve(indexref, wmodel=\"BM25\")\n",
    "dph = pt.BatchRetrieve(indexref, wmodel=\"DPH\")\n",
    "linear = 0.5*bm25+tf_idf + 2* dph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bm252 = pt.BatchRetrieve(indexref, wmodel=\"BM25\") >> pt.pipelines.PerQueryMaxMinScoreTransformer()\n",
    "dph2 = pt.BatchRetrieve(indexref, wmodel=\"DPH\") >> pt.pipelines.PerQueryMaxMinScoreTransformer()\n",
    "tf_idf2= pt.BatchRetrieve(indexref, wmodel=\"TF_IDF\") >> pt.pipelines.PerQueryMaxMinScoreTransformer()\n",
    "linear2 = 0.25 * tf_idf2 + 0.25*bm252 + 0.5 * dph2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pt.rewrite.RM3(indexref)\n",
    "#DPH qe best so far\n",
    "dph_qe = pt.BatchRetrieve(indexref, wmodel=\"DPH\", controls={\"qe\":\"on\", \"qemodel\" : \"KLCorrect\"},)\n",
    "dph_qe2 = pt.BatchRetrieve(indexref, wmodel=\"DPH\", controls={\"qe\":\"on\", \"qemodel\" : \"Bo1\"},)\n",
    "linear3 = 0.5*dph_qe + dph\n",
    "#mdl = pt.BatchRetrieve(indexref, wmodel=\"PL2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = (pt.BatchRetrieve(indexref, wmodel=\"DPH\", controls={\"qe\":\"on\", \"qemodel\" : \"KLCorrect\"}) >> \n",
    "    pt.rewrite.RM3(indexref) >> \n",
    "    pt.BatchRetrieve(indexref, wmodel=\"DPH\", controls={\"qe\":\"on\", \"qemodel\" : \"KLCorrect\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we do the retrieval...\n",
      "Done. Here are the first 10 entries of the run\n",
      "The run file is normalized outside the TIRA sandbox, I will store it at \"../runs/anotherExperiment\".\n",
      "Done. run file is stored under \"../runs/anotherExperiment/run.txt\".\n"
     ]
    }
   ],
   "source": [
    "pt_dataset.get_topics('text').head(3)\n",
    "\n",
    "print('Now we do the retrieval...')\n",
    "run = dph_qe(pt_dataset.get_topics('text'))\n",
    "\n",
    "print('Done. Here are the first 10 entries of the run')\n",
    "run.head(10)\n",
    "\n",
    "persist_and_normalize_run(run, system_name='altered index 1', default_output='../runs/anotherExperiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
